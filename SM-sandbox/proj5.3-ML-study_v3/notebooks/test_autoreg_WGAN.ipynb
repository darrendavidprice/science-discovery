{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing standard library\n",
      "Importing python data libraries\n",
      "Importing standard library\n",
      "Importing python data libraries\n",
      "Importing custom backends\n",
      "Importing keras objects\n",
      "Importing custom backends\n",
      "Importing keras objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing keras backend\n",
      "Importing keras backend\n"
     ]
    }
   ],
   "source": [
    "#  Required imports\n",
    "print(\"Importing standard library\")\n",
    "import os, time, sys\n",
    "\n",
    "print(\"Importing python data libraries\")\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Importing custom backends\")\n",
    "from   backends.stats import special_whiten_dataset, special_unwhiten_dataset\n",
    "from   backends.utils import joint_shuffle, make_sure_dir_exists_for_filename\n",
    "from   backends.ParameterisedSimulator import ParameterisedSimulator, Simulator_Model3\n",
    "from   backends.SamplingSimulator      import SamplingSimulator\n",
    "\n",
    "print(\"Importing keras objects\")\n",
    "from keras.layers      import BatchNormalization, Dense, Dropout, Input, LeakyReLU, Concatenate, Lambda, Reshape\n",
    "from keras.models      import Model, Sequential\n",
    "from keras.optimizers  import Adam, SGD, RMSprop\n",
    "from keras.constraints import Constraint\n",
    "from keras.callbacks   import EarlyStopping\n",
    "\n",
    "print(\"Importing keras backend\")\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Program constants\n",
    "\n",
    "mu_scan_points = np.linspace(-2, 3, 10)\n",
    "\n",
    "n_gen_points_per_c_per_ds        = 50000\n",
    "n_train_points_per_c_per_ds_true = 50000\n",
    "n_train_points_per_c_per_ds_fake = 50000\n",
    "use_Adam_optimiser = True\n",
    "\n",
    "train_batch_size            = 1000\n",
    "max_epochs                  = 1000\n",
    "\n",
    "do_whiten_data = True\n",
    "white_linear_fraction = 0.5\n",
    "\n",
    "plot_tag = None\n",
    "\n",
    "axis_lims = [(50, 250), (50, 500), (-1*np.pi, np.pi)]\n",
    "if do_whiten_data : axis_lims = [(-5., 5.), (-5., 5.), (-5., 5.)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets generated for scan points: -2.000, -1.444, -0.889, -0.333, 0.000, 0.222, 0.778, 1.333, 1.889, 2.444, 3.000\n",
      "Datasets generated for scan points: -2.000, -1.444, -0.889, -0.333, 0.000, 0.222, 0.778, 1.333, 1.889, 2.444, 3.000\n"
     ]
    }
   ],
   "source": [
    "#  Set up \"true\" model\n",
    "model = Simulator_Model3\n",
    "\n",
    "#  Generate several scan points for \"true\" model\n",
    "xsections, datasets, weights = {}, {}, {}\n",
    "for mu in mu_scan_points :\n",
    "    model.set_param_value(\"c\", mu)\n",
    "    xsec, dataset = model.generate(n_gen_points_per_c_per_ds)\n",
    "    xsections [mu] = xsec\n",
    "    datasets  [mu] = dataset\n",
    "    weights   [mu] = np.full(shape=(len(dataset),), fill_value=1./n_gen_points_per_c_per_ds)\n",
    "    \n",
    "#  Make sure one of the datasets was the SM\n",
    "model.set_param_value(\"c\", 0)\n",
    "if 0 not in xsections :\n",
    "    xsections [0], datasets [0] = model.generate(n_gen_points_per_c_per_ds)\n",
    "    mu_scan_points = np.sort(np.concatenate([mu_scan_points, [0]]))\n",
    "    weights [0] = np.full(shape=(len(dataset),), fill_value=1./n_gen_points_per_c_per_ds)\n",
    "xsec_SM, dataset_SM, weights_SM = xsections[0], datasets[0], weights[0]\n",
    "\n",
    "#  Whiten the data using the \"special\" hard-boundary-respecting method\n",
    "if do_whiten_data :\n",
    "    white_dataset_SM, whitening_funcs, whitening_params = special_whiten_dataset (dataset_SM,\n",
    "                                                                                  [50, 250, 201, white_linear_fraction],\n",
    "                                                                                  [50, 500, 301, white_linear_fraction],\n",
    "                                                                                  [-1.*np.pi, np.pi, 101, white_linear_fraction],\n",
    "                                                                                  rotate=True)\n",
    "else :\n",
    "    white_dataset_SM, whitening_funcs, whitening_params = dataset_SM, None, None\n",
    "\n",
    "def whiten_data (dataset) :\n",
    "    if do_whiten_data == False : return dataset\n",
    "    white_datasets = {}\n",
    "    for mu in dataset :\n",
    "        white_datasets [mu], _, _ = special_whiten_dataset (dataset[mu], \n",
    "                                                            whitening_funcs =whitening_funcs, \n",
    "                                                            whitening_params=whitening_params,\n",
    "                                                            rotate=True)\n",
    "    return white_datasets\n",
    "\n",
    "def unwhiten_data (dataset) : \n",
    "    if do_whiten_data == False : return dataset \n",
    "    unwhite_datasets = {}\n",
    "    for mu in dataset :\n",
    "        unwhite_datasets [mu] = special_unwhiten_dataset (dataset[mu], \n",
    "                                                          whitening_funcs =whitening_funcs, \n",
    "                                                          whitening_params=whitening_params)\n",
    "    return unwhite_datasets\n",
    "        \n",
    "white_datasets   = whiten_data  (datasets      )\n",
    "unwhite_datasets = unwhiten_data(white_datasets)\n",
    "num_datasets = len(white_datasets)\n",
    "print(f\"Datasets generated for scan points: {', '.join([f'{mu:.3f}' for mu in mu_scan_points])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset (mu_scan_points, xsections, datasets, weights=None, ref=None) :\n",
    "    num_datasets = len(mu_scan_points)\n",
    "           \n",
    "    if type(weights) == type(None) :\n",
    "        tmp_weights = {mu:np.full(fill_value=1./len(datasets[mu]), shape=(len(datasets[mu]),)) for mu in mu_scan_points}\n",
    "    elif type(weights) == np.ndarray :\n",
    "        tmp_weights = {mu:weights/np.sum(weights) for mu in mu_scan_points}\n",
    "    elif type(weights) == dict : \n",
    "        tmp_weights = {mu:tmp_weights/np.sum(tmp_weights) for mu, tmp_weights in weights.items()}\n",
    "    else :\n",
    "        raise TypeError(f\"Don't know what to do with weights of type {type(weights)}\")\n",
    "    \n",
    "    plot_reference = False\n",
    "    if type(ref) != type(None) :\n",
    "        plot_reference = True\n",
    "        xsections_ref, datasets_ref, weights_ref = ref\n",
    "        \n",
    "        if type(datasets_ref) == np.ndarray :\n",
    "            tmp_datasets_ref = {mu:datasets_ref for mu in mu_scan_points}\n",
    "        elif type(datasets_ref) == dict : \n",
    "            tmp_datasets_ref = datasets_ref\n",
    "        else :\n",
    "            raise TypeError(f\"Don't know what to do with reference dataset of type {type(datasets_ref)}\")\n",
    "            \n",
    "        if type(weights_ref) == type(None) :\n",
    "            tmp_weights_ref = {mu:np.full(fill_value=1/len(tmp_datasets_ref[mu]), shape=(len(tmp_datasets_ref[mu]),)) for mu in mu_scan_points}\n",
    "        elif type(weights_ref) == np.ndarray :\n",
    "            tmp_weights_ref = {mu:weights_ref/np.sum(weights_ref) for mu in mu_scan_points}\n",
    "        elif type(weights_ref) == dict : \n",
    "            tmp_weights_ref = {mu:tmp_weights_ref/np.sum(tmp_weights_ref) for mu, tmp_weights_ref in weights_ref.items()}\n",
    "        else :\n",
    "            raise TypeError(f\"Don't know what to do with reference weights of type {type(weights_ref)}\")\n",
    "            \n",
    "        if type(xsections_ref) == dict : \n",
    "            tmp_datasets_ref = datasets_ref\n",
    "        else :\n",
    "            tmp_datasets_ref = {mu:datasets_ref for mu in mu_scan_points}\n",
    "        \n",
    "        if type(xsections_ref) == type(None) :\n",
    "            raise RuntimeError(f\"Reference datasets must be provided reference cross sections too\")\n",
    "        if type(datasets_ref) == dict : \n",
    "            tmp_xsections_ref = xsections_ref\n",
    "        else :\n",
    "            tmp_xsections_ref = {mu:xsections_ref for mu in mu_scan_points}\n",
    "\n",
    "    num_plot_rows = 6\n",
    "    if plot_reference : num_plot_rows = 9\n",
    "    \n",
    "    fig = plt.figure(figsize=(4*num_datasets, 4*num_plot_rows))\n",
    "    plot_row_idx = 0\n",
    "    for ax_idx, mu in enumerate(mu_scan_points) :\n",
    "        dataset = datasets  [mu]\n",
    "        xsec    = xsections [mu]\n",
    "        weight  = tmp_weights [mu]\n",
    "        ax1 = fig.add_subplot(num_plot_rows, num_datasets, plot_row_idx*num_datasets + ax_idx + 1)\n",
    "        ax1.hist(dataset[:,0], alpha=0.5, weights=xsec*weight, fill=False, edgecolor=\"k\", linestyle=\"-\", linewidth=3)\n",
    "        if plot_reference :\n",
    "            dataset_ref = tmp_datasets_ref [mu]\n",
    "            xsec_ref    = tmp_xsections_ref [mu]\n",
    "            weight_ref  = tmp_weights_ref [mu]\n",
    "            ax1.hist(dataset_ref[:,0], alpha=0.5, weights=xsec_ref*weight_ref, fill=True, color=\"r\", linestyle=\"-\", linewidth=3)\n",
    "        ax1.set_title(f\"$c = {mu:.3f}$\", fontsize=30)\n",
    "        if ax_idx > 0 : continue\n",
    "        ax1.set_ylabel(r\"$\\frac{d\\sigma}{dA}$\", fontsize=30, rotation=0, labelpad=40)\n",
    "\n",
    "    plot_row_idx = plot_row_idx + 1\n",
    "    for ax_idx, mu in enumerate(mu_scan_points) :\n",
    "        dataset = datasets  [mu]\n",
    "        xsec    = xsections [mu]\n",
    "        weight  = tmp_weights [mu]\n",
    "        ax1 = fig.add_subplot(num_plot_rows, num_datasets, plot_row_idx*num_datasets + ax_idx + 1)\n",
    "        ax1.hist(dataset[:,1], alpha=0.5, weights=xsec*weight, fill=False, edgecolor=\"k\", linestyle=\"-\" , linewidth=3)\n",
    "        if plot_reference :\n",
    "            dataset_ref = tmp_datasets_ref [mu]\n",
    "            xsec_ref    = tmp_xsections_ref [mu]\n",
    "            weight_ref  = tmp_weights_ref [mu]\n",
    "            ax1.hist(dataset_ref[:,1], alpha=0.5, weights=xsec_ref*weight_ref, fill=True, color=\"r\", linestyle=\"-\", linewidth=3)\n",
    "        if ax_idx > 0 : continue\n",
    "        ax1.set_ylabel(r\"$\\frac{d\\sigma}{dB}$\", fontsize=30, rotation=0, labelpad=40)\n",
    "\n",
    "    plot_row_idx = plot_row_idx + 1\n",
    "    for ax_idx, mu in enumerate(mu_scan_points) :\n",
    "        dataset = datasets  [mu]\n",
    "        xsec    = xsections [mu]\n",
    "        weight  = tmp_weights [mu]\n",
    "        ax1 = fig.add_subplot(num_plot_rows, num_datasets, plot_row_idx*num_datasets + ax_idx + 1)\n",
    "        ax1.hist(dataset[:,2], alpha=0.5, weights=xsec*weight, fill=False, edgecolor=\"k\", linestyle=\"-\" , linewidth=3)\n",
    "        if plot_reference :\n",
    "            dataset_ref = tmp_datasets_ref [mu]\n",
    "            xsec_ref    = tmp_xsections_ref [mu]\n",
    "            weight_ref  = tmp_weights_ref [mu]\n",
    "            ax1.hist(dataset_ref[:,2], alpha=0.5, weights=xsec_ref*weight_ref, fill=True, color=\"r\", linestyle=\"-\", linewidth=3)\n",
    "        if ax_idx > 0 : continue\n",
    "        ax1.set_ylabel(r\"$\\frac{d\\sigma}{dC}$\", fontsize=30, rotation=0, labelpad=40)\n",
    "\n",
    "    plot_row_idx = plot_row_idx + 1\n",
    "    for ax_idx, mu in enumerate(mu_scan_points) :\n",
    "        dataset = datasets [mu]\n",
    "        weight  = tmp_weights [mu]\n",
    "        ax1 = fig.add_subplot(num_plot_rows, num_datasets, plot_row_idx*num_datasets + ax_idx + 1)\n",
    "        ax1.hist2d(dataset[:,0], dataset[:,1], weights=weight)\n",
    "        if ax_idx == 0 : \n",
    "            ax1.set_ylabel(\"$A$ \\n / \\n $B$\", fontsize=30, rotation=0, labelpad=40)\n",
    "    \n",
    "    if plot_reference :\n",
    "        plot_row_idx = plot_row_idx + 1\n",
    "        for ax_idx, mu in enumerate(mu_scan_points) :\n",
    "            dataset = tmp_datasets_ref [mu]\n",
    "            weight  = tmp_weights_ref [mu]\n",
    "            ax1 = fig.add_subplot(num_plot_rows, num_datasets, plot_row_idx*num_datasets + ax_idx + 1)\n",
    "            ax1.hist2d(dataset[:,0], dataset[:,1], weights=weight)\n",
    "            if ax_idx == 0 : \n",
    "                ax1.set_ylabel(\"$A$ \\n / \\n $B$ \\n ref\", fontsize=30, rotation=0, labelpad=40, va=\"center\")\n",
    "\n",
    "    plot_row_idx = plot_row_idx + 1\n",
    "    for ax_idx, mu in enumerate(mu_scan_points) :\n",
    "        dataset = datasets [mu]\n",
    "        weight  = tmp_weights [mu]\n",
    "        ax1 = fig.add_subplot(num_plot_rows, num_datasets, plot_row_idx*num_datasets + ax_idx + 1)\n",
    "        ax1.hist2d(dataset[:,0], dataset[:,2], weights=weight)\n",
    "        if ax_idx == 0 : \n",
    "            ax1.set_ylabel(\"$A$ \\n / \\n $C$\", fontsize=30, rotation=0, labelpad=40)\n",
    "    \n",
    "    if plot_reference :\n",
    "        plot_row_idx = plot_row_idx + 1\n",
    "        for ax_idx, mu in enumerate(mu_scan_points) :\n",
    "            dataset = tmp_datasets_ref [mu]\n",
    "            weight  = tmp_weights_ref [mu]\n",
    "            ax1 = fig.add_subplot(num_plot_rows, num_datasets, plot_row_idx*num_datasets + ax_idx + 1)\n",
    "            ax1.hist2d(dataset[:,0], dataset[:,2], weights=weight)\n",
    "            if ax_idx == 0 : \n",
    "                ax1.set_ylabel(\"$A$ \\n / \\n $C$ \\n ref\", fontsize=30, rotation=0, labelpad=40, va=\"center\")\n",
    "\n",
    "    plot_row_idx = plot_row_idx + 1\n",
    "    for ax_idx, mu in enumerate(mu_scan_points) :\n",
    "        dataset = datasets [mu]\n",
    "        weight  = tmp_weights [mu]\n",
    "        ax1 = fig.add_subplot(num_plot_rows, num_datasets, plot_row_idx*num_datasets + ax_idx + 1)\n",
    "        ax1.hist2d(dataset[:,1], dataset[:,2], weights=weight)\n",
    "        if ax_idx == 0 : \n",
    "            ax1.set_ylabel(\"$B$ \\n / \\n $C$\", fontsize=30, rotation=0, labelpad=40)\n",
    "    \n",
    "    if plot_reference :\n",
    "        plot_row_idx = plot_row_idx + 1\n",
    "        for ax_idx, mu in enumerate(mu_scan_points) :\n",
    "            dataset = tmp_datasets_ref [mu]\n",
    "            weight  = tmp_weights_ref [mu]\n",
    "            ax1 = fig.add_subplot(num_plot_rows, num_datasets, plot_row_idx*num_datasets + ax_idx + 1)\n",
    "            ax1.hist2d(dataset[:,1], dataset[:,2], weights=weight)\n",
    "            if ax_idx == 0 : \n",
    "                ax1.set_ylabel(\"$B$ \\n / \\n $C$ \\n ref\", fontsize=30, rotation=0, labelpad=40, va=\"center\")\n",
    "                \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Plotting datasets\")\n",
    "#plot_dataset(mu_scan_points, xsections, datasets, weights, ref=(xsec_SM, dataset_SM, weights_SM))\n",
    "plot_dataset(mu_scan_points, xsections, datasets, weights)\n",
    "\n",
    "print(\"Plotting unwhitened(whitened(datasets)) to show this reconstructs original datasets\")\n",
    "#plot_dataset(mu_scan_points, xsections, unwhiten_data(white_datasets), weights, ref=(xsec_SM, dataset_SM, weights_SM))\n",
    "plot_dataset(mu_scan_points, xsections, unwhiten_data(white_datasets), weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting whitened datasets\")\n",
    "#plot_dataset(mu_scan_points, xsections, white_datasets, weights, ref=(xsec_SM, white_dataset_SM, weights_SM))\n",
    "plot_dataset(mu_scan_points, xsections, white_datasets, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "# clip model weights to a given hypercube\n",
    "class ClipConstraint(Constraint):\n",
    "    def __init__(self, clip_value):\n",
    "        self.clip_value = clip_value\n",
    "    def __call__(self, weights):\n",
    "        return K.clip(weights, -self.clip_value, self.clip_value)\n",
    "    def get_config(self):\n",
    "        return {'clip_value': self.clip_value}\n",
    "\n",
    "def create_autoreg_wgan_segment (name, **kwargs) :\n",
    "    #  Parse arguments\n",
    "    #\n",
    "    layer_idx      = int (kwargs.get(\"layer_idx\"     ))\n",
    "    num_conditions = int (kwargs.get(\"num_conditions\"))\n",
    "    verbose        = bool(kwargs.get(\"verbose\" , True))\n",
    "    prev_generator = kwargs.get(\"prev_generator\"  , None)\n",
    "    \n",
    "    #  Print a status message\n",
    "    #\n",
    "    if verbose : \n",
    "        print(f\"Creating autoregressive WGAN segment: {name}\")\n",
    "        print(f\"  - layer_idx      is {layer_idx}\")\n",
    "        print(f\"  - num_conditions is {num_conditions}\")\n",
    "      \n",
    "    #  Create Inputs for the GAN\n",
    "    #  \n",
    "    condition_input   = Input((num_conditions,))\n",
    "    critic_data_input = Input((layer_idx+1,))\n",
    "    gen_noise_input   = Input((layer_idx+1,))\n",
    "    \n",
    "    #  Create critic\n",
    "    #\n",
    "    critic_data      = Dense    (20 , kernel_constraint=ClipConstraint(0.02))(critic_data_input)\n",
    "    critic_data      = LeakyReLU(0.2)(critic_data      )\n",
    "    critic_condition = Dense    (20 , kernel_constraint=ClipConstraint(0.02))(condition_input  )\n",
    "    critic_condition = LeakyReLU(0.2)(critic_condition )\n",
    "    critic           = Concatenate()([critic_data, critic_condition])\n",
    "    critic           = Dropout(0.1)           (critic)\n",
    "    critic           = Dense             (50 , kernel_constraint=ClipConstraint(0.02))(critic)\n",
    "    critic           = LeakyReLU         (0.2)(critic)\n",
    "    critic           = Dropout(0.1)           (critic)\n",
    "    critic           = Dense             (50 , kernel_constraint=ClipConstraint(0.02))(critic)\n",
    "    critic           = LeakyReLU         (0.2)(critic)\n",
    "    critic           = Dropout(0.1)           (critic)\n",
    "    critic           = Dense             (50 , kernel_constraint=ClipConstraint(0.02))(critic)\n",
    "    critic           = LeakyReLU         (0.2)(critic)\n",
    "    critic           = Dropout(0.1)           (critic)\n",
    "    critic           = Dense             (1  , activation=\"linear\")(critic)\n",
    "    critic           = Model(name=name+\"_critic\", \n",
    "                                   inputs=[critic_data_input, condition_input], \n",
    "                                   outputs=[critic])\n",
    "    \n",
    "    if use_Adam_optimiser :\n",
    "        critic.compile(loss=wasserstein_loss, optimizer=Adam())\n",
    "    else :\n",
    "        critic.compile(loss=wasserstein_loss, optimizer=RMSprop(learning_rate=5e-6, rho=0))\n",
    "    if verbose : critic.summary()\n",
    "    \n",
    "    #  Process Input objects into the first layers of the layer_idx'th generator\n",
    "    #\n",
    "    if layer_idx == 0 :\n",
    "        gen_observable_noise = gen_noise_input\n",
    "        gen_all_conditions   = condition_input\n",
    "    else :\n",
    "        gen_observable_noise = Lambda(lambda x: x[:, layer_idx])(gen_noise_input)\n",
    "        gen_observable_noise = Reshape((1,))(gen_observable_noise)\n",
    "        prev_gen_noise       = Lambda(lambda x: x[:,:layer_idx])(gen_noise_input)\n",
    "        prev_gen_noise       = Reshape((layer_idx,))(prev_gen_noise)\n",
    "        prev_observables     = prev_generator([prev_gen_noise, condition_input])\n",
    "        gen_all_conditions   = Concatenate()([condition_input, prev_observables])                \n",
    "\n",
    "    #  Create generator\n",
    "    #\n",
    "    generator_noise     = Dense    (20   )(gen_observable_noise)\n",
    "    generator_noise     = LeakyReLU(0.2  )(generator_noise     )\n",
    "    generator_condition = Dense    (20   )(gen_all_conditions  )\n",
    "    generator_condition = LeakyReLU(0.2  )(generator_condition )\n",
    "    generator           = Concatenate()([generator_noise, generator_condition])\n",
    "    #generator           = BatchNormalization()            (generator)\n",
    "    generator           = Dropout(0.1                    )(generator)\n",
    "    generator           = Dense  (50                     )(generator)\n",
    "    generator           = LeakyReLU(0.2                  )(generator)\n",
    "    #generator           = BatchNormalization()            (generator)\n",
    "    generator           = Dropout(0.1                    )(generator)\n",
    "    generator           = Dense  (50,                    )(generator)\n",
    "    generator           = LeakyReLU(0.2                  )(generator)\n",
    "    generator           = Dropout(0.1                    )(generator)\n",
    "    generator           = Dense  (50,                    )(generator)\n",
    "    generator           = LeakyReLU(0.2                  )(generator)\n",
    "    generator           = Dropout(0.1                    )(generator)\n",
    "    generator           = Dense  (1 , activation=\"linear\")(generator)\n",
    "    if layer_idx > 0 : generator = Concatenate()([prev_observables, generator])\n",
    "    generator           = Model(name=name+\"_generator\", \n",
    "                                inputs=[gen_noise_input, condition_input], \n",
    "                                outputs=[generator])\n",
    "    if verbose : generator.summary()\n",
    "\n",
    "    GAN = critic([generator([gen_noise_input, condition_input]), condition_input])\n",
    "    GAN = Model([gen_noise_input, condition_input], GAN, name=name)\n",
    "    critic.trainable = False\n",
    "    \n",
    "    if use_Adam_optimiser :\n",
    "        GAN.compile(loss=wasserstein_loss, optimizer=Adam())\n",
    "    else :\n",
    "        GAN.compile(loss=wasserstein_loss, optimizer=RMSprop(learning_rate=5e-6, rho=0))\n",
    "    if verbose : GAN.summary()\n",
    "        \n",
    "    return critic, generator, GAN\n",
    "           \n",
    "def create_autoreg_wgan (name, **kwargs) :\n",
    "    num_conditions  = int (kwargs.get(\"num_conditions\" ))\n",
    "    num_observables = int (kwargs.get(\"num_observables\"))\n",
    "    verbose         = bool(kwargs.get(\"verbose\"  , True))\n",
    "\n",
    "    if verbose : \n",
    "        print(f\"Creating WGAN: {name}\")\n",
    "        print(f\"  - num_observables is {num_observables}\")\n",
    "        print(f\"  - num_conditions  is {num_conditions}\")\n",
    "    \n",
    "    layer_critics, layer_generators, layer_GANs = [], [], []\n",
    "    for i in range(num_observables):\n",
    "        prev_generator = None\n",
    "        if len(layer_generators) > 0 : prev_generator = layer_generators[-1]\n",
    "        critic_i, generator_i, GAN_i = create_autoreg_wgan_segment(name              = f\"{name}_observable{i}\",\n",
    "                                                                   layer_idx         = i,\n",
    "                                                                   num_conditions    = num_conditions,\n",
    "                                                                   prev_generator    = prev_generator,\n",
    "                                                                   verbose           = verbose)\n",
    "        critic_i.trainable, generator_i.trainable = False, False\n",
    "        layer_critics             .append(critic_i                  )\n",
    "        layer_generators          .append(generator_i               )\n",
    "        layer_GANs                .append(GAN_i                     )\n",
    "    \n",
    "    return layer_critics, layer_generators, layer_GANs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "layer_critics, layer_generators, layer_GANs = create_autoreg_wgan(\"autoreg_GAN\", \n",
    "                                                                  num_conditions=1, \n",
    "                                                                  num_observables=3,\n",
    "                                                                  verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise (batch_size, train_points_c, GAN_noise_size) :\n",
    "    hyperparams = np.concatenate([np.full(fill_value=c, shape=(batch_size, 1)) for c in train_points_c])\n",
    "    return np.random.normal(size=(batch_size*len(train_points_c), GAN_noise_size)), hyperparams\n",
    "\n",
    "def get_train_data (batch_size, train_points_c, datasets, max_axis) :\n",
    "    data_batch  = np.concatenate([ds[np.random.randint(0, len(ds), batch_size),:max_axis] for c, ds in datasets.items()])\n",
    "    if len(data_batch.shape) == 1 :\n",
    "        data_batch = data_batch.reshape((data_batch.shape[0],1))\n",
    "    hyperparams = np.concatenate([np.full(fill_value=c, shape=(batch_size, 1)) for c in train_points_c])\n",
    "    return data_batch, hyperparams\n",
    "    \n",
    "def get_train_fakes (batch_size, train_points_c, GAN_noise_size, generator) :\n",
    "    noise, hyperparams = list(get_noise(batch_size, train_points_c, GAN_noise_size))\n",
    "    fakes_batch = generator.predict([noise, hyperparams])\n",
    "    return fakes_batch, hyperparams\n",
    "\n",
    "'''def get_batch_size (epoch_idx, min_batch_size, max_batch_size, batch_update_per_epoch) :\n",
    "    return int(np.min([min_batch_size + batch_update_per_epoch*epoch_idx, max_batch_size]))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_test_stat (ds1, ds2) :\n",
    "    E1, E2, E3 = 0., 0., 0.\n",
    "    n1, n2 = len(ds1), len(ds2)\n",
    "    for x1 in ds1 :\n",
    "        for x2 in ds2 :\n",
    "            res = x2 - x1\n",
    "            E1 = E1 + np.matmul(res, res)\n",
    "    E1 = np.sqrt(E1) / n1 / n2\n",
    "    for x1 in ds1 :\n",
    "        for x2 in ds1 :\n",
    "            res = x2 - x1\n",
    "            E2 = E2 + np.matmul(res, res)\n",
    "    E2 = np.sqrt(E2) / n1 / n1\n",
    "    for x1 in ds2 :\n",
    "        for x2 in ds2 :\n",
    "            res = x2 - x1\n",
    "            E3 = E3 + np.matmul(res, res)\n",
    "    E3 = np.sqrt(E3) / n1 / n1\n",
    "    return 2*E1 - E2 - E3\n",
    "\n",
    "def get_Et_thresholds (batch_size, threshold, num_toys=10) :\n",
    "    toy_energy_values = {}\n",
    "    for mu in mu_scan_points :\n",
    "        toy_energy_values [mu] = []\n",
    "    \n",
    "    sys.stdout.write(\"Throwing toys...\")\n",
    "    for i in range(num_toys) :\n",
    "        sys.stdout.write(f\"\\rThrowing {num_toys} toys... [{int(100.*(i+1)/num_toys)}%]\")\n",
    "        toy1_data , toy1_data_conditions  = get_train_data (batch_size, mu_scan_points, white_datasets, max_axis=1)\n",
    "        toy2_data , toy2_data_conditions  = get_train_data (batch_size, mu_scan_points, white_datasets, max_axis=1)\n",
    "        for mu in mu_scan_points :\n",
    "            toy1_data_mu  = np.array([x for x,c in zip(toy1_data, toy1_data_conditions) if c == mu])\n",
    "            toy2_data_mu  = np.array([x for x,c in zip(toy2_data, toy2_data_conditions) if c == mu])\n",
    "            toy_energy_values[mu].append(energy_test_stat(toy1_data_mu, toy2_data_mu))\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    \n",
    "    num_mu_scan_points = len(mu_scan_points)\n",
    "    \n",
    "    fig = plt.figure(figsize=(2.5*num_mu_scan_points, 2))\n",
    "    Et_thresholds = {}\n",
    "    for ax_idx, mu in enumerate(mu_scan_points) :\n",
    "        vals = sorted(toy_energy_values[mu])\n",
    "        Et_threshold      = vals[min(int(threshold*len(vals)), len(vals)-1)]\n",
    "        Et_thresholds[mu] = Et_threshold\n",
    "        ax = fig.add_subplot(1, num_mu_scan_points, 1+ax_idx)\n",
    "        ax.hist(vals, color=\"green\", alpha=0.5)\n",
    "        ax.axvline(Et_threshold, linestyle=\"--\", c=\"k\", linewidth=2)\n",
    "        ax.set_title(f\"$\\mu = {mu:.2f}$\")\n",
    "        ax.set_xlabel(\"Energy test\", fontsize=12, labelpad=15)\n",
    "    plt.show()\n",
    "                \n",
    "    return Et_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_batch_size, batch_update_factor, max_batch_size = 60, 1.5, 1000\n",
    "max_epochs                   = 100000000\n",
    "max_critic_updates_per_epoch = 10\n",
    "max_gen_updates_per_epoch    = 1\n",
    "\n",
    "epoch_print_interval = 200\n",
    "\n",
    "energy_test_threshold = 1\n",
    "energy_test_ntoys     = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generator, critic, GAN = layer_generators[0], layer_critics[0], layer_GANs[0]\n",
    "\n",
    "epoch_idx = -1\n",
    "axis_idx  = 0\n",
    "batch_size = initial_batch_size\n",
    "print(\"Getting initial Et thresholds\")\n",
    "Et_thresholds = get_Et_thresholds(batch_size, threshold=energy_test_threshold, num_toys=energy_test_ntoys)\n",
    "saved_epochs, saved_energy_tests, epoch_batch_size_transitions = [], {}, []\n",
    "while epoch_idx < max_epochs :\n",
    "    epoch_idx = epoch_idx + 1\n",
    "    \n",
    "    num_conditions = len(mu_scan_points)\n",
    "    data_batch , data_conditions  = get_train_data (batch_size, mu_scan_points, white_datasets, max_axis=axis_idx+1)\n",
    "    fakes_batch, fakes_conditions = get_train_fakes(batch_size, mu_scan_points, axis_idx+1, generator)\n",
    "    \n",
    "    data_labels = np.array([1.  for i in range(num_conditions*batch_size)])\n",
    "    fake_labels = np.array([-1. for i in range(num_conditions*batch_size)])\n",
    "    \n",
    "    critic_train_X_datapoints = np.concatenate([data_batch     , fakes_batch     ])\n",
    "    critic_train_X_conditions = np.concatenate([data_conditions, fakes_conditions])\n",
    "    critic_train_Y_labels     = np.concatenate([data_labels    , fake_labels     ])\n",
    "    critic_train_X_datapoints, critic_train_X_conditions, critic_train_Y_labels = joint_shuffle(critic_train_X_datapoints, critic_train_X_conditions, critic_train_Y_labels)\n",
    "    critic_train_X = [critic_train_X_datapoints, critic_train_X_conditions]\n",
    "    \n",
    "    # train critic\n",
    "    #print(f\"Training critic at epoch {epoch_idx}\")\n",
    "    critic.trainable = True\n",
    "    for i in range(max_critic_updates_per_epoch) :\n",
    "        critic.train_on_batch(critic_train_X, -1.*critic_train_Y_labels)\n",
    "    '''critic.fit(critic_train_X, \n",
    "               -1.*critic_train_Y_labels,\n",
    "               shuffle=True,\n",
    "               validation_split=0.3,\n",
    "               epochs=max_critic_updates_per_epoch,\n",
    "               callbacks=[EarlyStopping(restore_best_weights=True, \n",
    "                                        monitor=\"val_loss\", \n",
    "                                        patience=3)]\n",
    "               )'''\n",
    "    \n",
    "    # train generator\n",
    "    #print(f\"Training generator at epoch {epoch_idx}\")\n",
    "    new_noise, new_conditions = get_noise (batch_size, mu_scan_points, axis_idx+1)\n",
    "    generator_train_X = [new_noise, new_conditions]\n",
    "    critic   .trainable = False\n",
    "    generator.trainable = True\n",
    "    for i in range(max_critic_updates_per_epoch) :\n",
    "        GAN_loss = GAN.train_on_batch(generator_train_X, fake_labels)\n",
    "    '''GAN.fit(generator_train_X, \n",
    "            fake_labels,\n",
    "            shuffle=True,\n",
    "            validation_split=0.3,\n",
    "            epochs=max_gen_updates_per_epoch,\n",
    "            callbacks=[EarlyStopping(restore_best_weights=True, \n",
    "                                     monitor=\"val_loss\", \n",
    "                                     patience=3)]\n",
    "               )\n",
    "    GAN_loss = GAN.evaluate(generator_train_X, fake_labels)'''\n",
    "    if epoch_idx % epoch_print_interval != 0 : continue\n",
    "    print(f\"Epoch {epoch_idx} GAN loss is {GAN_loss}\")\n",
    "    print(f\"Batch size is {batch_size}\")\n",
    "    \n",
    "    data_batch , data_conditions  = get_train_data (batch_size, mu_scan_points, white_datasets, max_axis=axis_idx+1)\n",
    "    fakes_batch, fakes_conditions = get_train_fakes(batch_size, mu_scan_points, axis_idx+1, generator)\n",
    "    \n",
    "    num_mu_scan_points = len(mu_scan_points)\n",
    "    fig = plt.figure(figsize=(2.5*num_mu_scan_points, 2))\n",
    "    ax_lims = axis_lims[0]\n",
    "    update_batch_size = True if batch_size < max_batch_size else False\n",
    "    saved_epochs.append(epoch_idx+1)\n",
    "    for ax_idx, mu in enumerate(mu_scan_points) :\n",
    "        ax = fig.add_subplot(1, num_mu_scan_points, 1+ax_idx)\n",
    "        plot_data  = np.array([x for x,c in zip(data_batch , data_conditions ) if c == mu])\n",
    "        plot_data .reshape((plot_data.shape[0],))\n",
    "        plot_fakes = np.array([x for x,c in zip(fakes_batch, fakes_conditions) if c == mu])\n",
    "        plot_fakes.reshape((plot_fakes.shape[0],))\n",
    "        #ax.hist(plot_data , color=\"blue\", alpha=0.5)\n",
    "        #ax.hist(plot_fakes, color=\"red\" , alpha=0.5)\n",
    "        ax.hist(plot_data , bins=np.linspace(ax_lims[0], ax_lims[1], 21), color=\"blue\", alpha=0.5)\n",
    "        ax.hist(plot_fakes, bins=np.linspace(ax_lims[0], ax_lims[1], 21), color=\"red\" , alpha=0.5)\n",
    "        ax.text(0.95, 0.9, f\"{100.*len([x for x in plot_fakes if x > ax_lims[1]])/len(plot_data):.0f}% ->\", ha=\"right\", color=\"red\", transform=ax.transAxes)\n",
    "        ax.text(0.05, 0.9, f\"<- {100.*len([x for x in plot_fakes if x < ax_lims[0]])/len(plot_data):.0f}%\", ha=\"left\" , color=\"red\", transform=ax.transAxes)\n",
    "        ax.set_xlabel(\"Observable\", fontsize=12, labelpad=15)\n",
    "        ax.set_title(f\"$\\mu = {mu:.2f}$\")\n",
    "        Et           = energy_test_stat(plot_data, plot_fakes)\n",
    "        Et_threshold = Et_thresholds [mu]\n",
    "        print(f\"Energy test [mu = {mu:.2f}] = {Et:.2f}, with threshold of {Et_threshold:.2f}\")\n",
    "        if Et > Et_threshold : update_batch_size = False\n",
    "        if mu not in saved_energy_tests : saved_energy_tests [mu] = []\n",
    "        saved_energy_tests[mu].append(Et)\n",
    "    plt.show()\n",
    "    \n",
    "    fig = plt.figure(figsize=(2.5*num_mu_scan_points, 2))\n",
    "    for ax_idx, mu in enumerate(mu_scan_points) :\n",
    "        ax = fig.add_subplot(1, num_mu_scan_points, 1+ax_idx)\n",
    "        ax.plot(saved_epochs, saved_energy_tests[mu], \"x-\")\n",
    "        for v in epoch_batch_size_transitions :\n",
    "            ax.axvline(v[0], color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "            ax.set_yscale(\"log\")\n",
    "        ax.set_xlabel(\"Epoch\", fontsize=12, labelpad=15)\n",
    "        if ax_idx == 0 : ax.set_ylabel(\"Energy test\", fontsize=12, labelpad=15)\n",
    "        ax.set_title(f\"$\\mu = {mu:.2f}$\")\n",
    "    \n",
    "    if update_batch_size is False : continue\n",
    "        \n",
    "    print(f\"Updating batch factor and throwing toys for Et thresholds\")\n",
    "    batch_size    = np.min([max_batch_size, int(batch_size * batch_update_factor)])\n",
    "    epoch_batch_size_transitions.append([epoch_idx+1, batch_size])\n",
    "    Et_thresholds = get_Et_thresholds(batch_size, threshold=energy_test_threshold, num_toys=energy_test_ntoys)\n",
    "        \n",
    "    print(\"Continuing\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''critic.save_weights(\".critic0_1.h5\")\n",
    "generator.save_weights(\".generator0_1.h5\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch , data_conditions  = get_train_data (n_gen_points_per_c_per_ds, mu_scan_points, white_datasets, max_axis=axis_idx+1)\n",
    "fakes_batch, fakes_conditions = get_train_fakes(n_gen_points_per_c_per_ds, mu_scan_points, axis_idx+1, generator)\n",
    "    \n",
    "num_mu_scan_points = len(mu_scan_points)\n",
    "fig = plt.figure(figsize=(2.5*num_mu_scan_points, 2))\n",
    "update_batch_size = True if batch_size < max_batch_size else False\n",
    "for ax_idx, mu in enumerate(mu_scan_points) :\n",
    "    ax = fig.add_subplot(1, num_mu_scan_points, 1+ax_idx)\n",
    "    plot_data  = np.array([x for x,c in zip(data_batch , data_conditions ) if c == mu])\n",
    "    plot_data .reshape((plot_data.shape[0],))\n",
    "    plot_fakes = np.array([x for x,c in zip(fakes_batch, fakes_conditions) if c == mu])\n",
    "    plot_fakes.reshape((plot_fakes.shape[0],))\n",
    "    ax.hist(plot_data , bins=np.linspace(axis_lims[0][0], axis_lims[0][1], 51), color=\"blue\", alpha=0.5)\n",
    "    ax.hist(plot_fakes, bins=np.linspace(axis_lims[0][0], axis_lims[0][1], 51), color=\"red\" , alpha=0.5)\n",
    "    ax.set_xlabel(\"Observable\", fontsize=12, labelpad=15)\n",
    "    ax.set_title(f\"$\\mu = {mu:.2f}$\")\n",
    "    ax.set_ylim(0, n_gen_points_per_c_per_ds*8000/n_gen_points_per_c_per_ds)\n",
    "    ax.axvline(0, linestyle=\"--\", linewidth=1, c=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.trainable = False\n",
    "critic.trainable = False\n",
    "generator, critic, GAN = layer_generators[1], layer_critics[1], layer_GANs[1]\n",
    "\n",
    "epoch_idx = -1\n",
    "axis_idx  = 1\n",
    "batch_size = initial_batch_size\n",
    "print(\"Getting initial Et thresholds\")\n",
    "Et_thresholds = get_Et_thresholds(batch_size, threshold=energy_test_threshold, num_toys=energy_test_ntoys)\n",
    "while epoch_idx < max_epochs :\n",
    "    epoch_idx = epoch_idx + 1\n",
    "    \n",
    "    num_conditions = len(mu_scan_points)\n",
    "    data_batch , data_conditions  = get_train_data (batch_size, mu_scan_points, white_datasets, max_axis=axis_idx+1)\n",
    "    fakes_batch, fakes_conditions = get_train_fakes(batch_size, mu_scan_points, axis_idx+1, generator)\n",
    "    \n",
    "    data_labels = np.array([1.  for i in range(num_conditions*batch_size)])\n",
    "    fake_labels = np.array([-1. for i in range(num_conditions*batch_size)])\n",
    "    \n",
    "    critic_train_X_datapoints = np.concatenate([data_batch     , fakes_batch     ])\n",
    "    critic_train_X_conditions = np.concatenate([data_conditions, fakes_conditions])\n",
    "    critic_train_Y_labels     = np.concatenate([data_labels    , fake_labels     ])\n",
    "    critic_train_X_datapoints, critic_train_X_conditions, critic_train_Y_labels = joint_shuffle(critic_train_X_datapoints, critic_train_X_conditions, critic_train_Y_labels)\n",
    "    critic_train_X = [critic_train_X_datapoints, critic_train_X_conditions]\n",
    "    \n",
    "    # train critic\n",
    "    #print(f\"Training critic at epoch {epoch_idx}\")\n",
    "    critic.trainable = True\n",
    "    for i in range(max_critic_updates_per_epoch) :\n",
    "        critic.train_on_batch(critic_train_X, -1.*critic_train_Y_labels)\n",
    "    \n",
    "    # train generator\n",
    "    #print(f\"Training generator at epoch {epoch_idx}\")\n",
    "    new_noise, new_conditions = get_noise (batch_size, mu_scan_points, axis_idx+1)\n",
    "    generator_train_X = [new_noise, new_conditions]\n",
    "    critic   .trainable = False\n",
    "    generator.trainable = True\n",
    "    for i in range(max_critic_updates_per_epoch) :\n",
    "        GAN_loss = GAN.train_on_batch(generator_train_X, fake_labels)\n",
    "    if epoch_idx % epoch_print_interval != 0 : continue\n",
    "    print(f\"Epoch {epoch_idx} GAN loss is {GAN_loss}\")\n",
    "    print(f\"Batch size is {batch_size}\")\n",
    "    \n",
    "    data_batch , data_conditions  = get_train_data (batch_size, mu_scan_points, white_datasets, max_axis=axis_idx+1)\n",
    "    fakes_batch, fakes_conditions = get_train_fakes(batch_size, mu_scan_points, axis_idx+1, generator)\n",
    "    \n",
    "    num_mu_scan_points = len(mu_scan_points)\n",
    "    fig = plt.figure(figsize=(2.5*num_mu_scan_points, 4))\n",
    "    update_batch_size = True if batch_size < max_batch_size else False\n",
    "    for ax_idx, mu in enumerate(mu_scan_points) :\n",
    "        plot_data  = np.array([x for x,c in zip(data_batch , data_conditions ) if c == mu])\n",
    "        plot_fakes = np.array([x for x,c in zip(fakes_batch, fakes_conditions) if c == mu])\n",
    "        ax = fig.add_subplot(2, num_mu_scan_points, 1+ax_idx)\n",
    "        ax.hist2d(plot_data [:,0], plot_data [:,1], bins=[np.linspace(axis_lims[0][0], axis_lims[0][1], 21), np.linspace(axis_lims[1][0], axis_lims[1][1], 21)])\n",
    "        ax.set_title(f\"$\\mu = {mu:.2f}$\")\n",
    "        ax = fig.add_subplot(2, num_mu_scan_points, num_mu_scan_points+1+ax_idx)\n",
    "        ax.hist2d(plot_fakes[:,0], plot_fakes[:,1], bins=[np.linspace(axis_lims[0][0], axis_lims[0][1], 21), np.linspace(axis_lims[1][0], axis_lims[1][1], 21)])\n",
    "        ax.set_xlabel(\"Observable\", fontsize=12, labelpad=15)\n",
    "        if update_batch_size is False : continue\n",
    "        Et           = energy_test_stat(plot_data, plot_fakes)\n",
    "        Et_threshold = Et_thresholds [mu]\n",
    "        print(f\"Energy test [mu = {mu:.2f}] = {Et:.2f}, with threshold of {Et_threshold:.2f}\")\n",
    "        if Et > Et_threshold : update_batch_size = False\n",
    "    plt.show()\n",
    "    \n",
    "    if update_batch_size is False : continue\n",
    "        \n",
    "    print(f\"Updating batch factor and throwing toys for Et thresholds\")\n",
    "    batch_size    = np.min([max_batch_size, int(batch_size * batch_update_factor)])\n",
    "    Et_thresholds = get_Et_thresholds(batch_size, threshold=energy_test_threshold, num_toys=energy_test_ntoys)\n",
    "        \n",
    "    print(\"Continuing\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  train on observable 1\n",
    "\n",
    "epoch_idx = -1\n",
    "while True :\n",
    "    epoch_idx = epoch_idx + 1\n",
    "    \n",
    "    batch_size = get_batch_size(original_batch_size, max_batch_size, epoch_idx)\n",
    "    data_batch , data_conditions  = get_train_data (batch_size, train_points_c, datasets)\n",
    "    fakes_batch, fakes_conditions = get_train_fakes(batch_size, train_points_c, GAN_noise_size, generator)\n",
    "    \n",
    "    data_labels = np.array([1.  for i in range(num_conditions*batch_size)])\n",
    "    fake_labels = np.array([-1. for i in range(num_conditions*batch_size)])\n",
    "    \n",
    "    critic_train_X_datapoints = np.concatenate([data_batch     , fakes_batch     ])\n",
    "    critic_train_X_conditions = np.concatenate([data_conditions, fakes_conditions])\n",
    "    critic_train_Y_labels     = np.concatenate([data_labels    , fake_labels     ])\n",
    "    critic_train_X = [critic_train_X_datapoints, critic_train_X_conditions]\n",
    "    \n",
    "    # train discriminator\n",
    "    critic.trainable = True\n",
    "    for critich_update_itr in range(critic_itrs_per_generator_itr) :\n",
    "        critic_loss = critic.train_on_batch(critic_train_X, -1.*critic_train_Y_labels)\n",
    "    \n",
    "    # train generator\n",
    "    new_noise, new_conditions = get_noise (batch_size, train_points_c, GAN_noise_size)\n",
    "    generator_train_X = [new_noise, new_conditions]\n",
    "    critic   .trainable = False\n",
    "    generator.trainable = True\n",
    "    GAN_loss = GAN.train_on_batch(generator_train_X, fake_labels)\n",
    "    \n",
    "    if epoch_idx % epoch_print_interval != 0 : continue\n",
    "    print(f\"Epoch {epoch_idx} GAN loss is {GAN_loss}\")\n",
    "    \n",
    "    data_batch , data_conditions  = get_train_data (batch_size, train_points_c, datasets)\n",
    "    fakes_batch, fakes_conditions = get_train_fakes(batch_size, train_points_c, GAN_noise_size, generator)\n",
    "    plot_progress (train_points_c, data_batch, data_conditions, fakes_batch, fakes_conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_mean, A_sigma = 50, 100\n",
    "B_mean, B_sigma = 125, 8\n",
    "v_means, v_sigmas = np.array([A_mean, B_mean]), np.array([A_sigma, B_sigma])\n",
    "\n",
    "Amin, Amax, A_npoints = 50 , 300, 251\n",
    "Bmin, Bmax, B_npoints = 100, 150, 251\n",
    "\n",
    "rotate_angle = np.pi/4\n",
    "rotate = np.array([[np.cos(rotate_angle), -1.*np.sin(rotate_angle)], [np.sin(rotate_angle), np.cos(rotate_angle)]])\n",
    "cov = np.zeros(shape=rotate.shape)\n",
    "for i in range(2) :\n",
    "    for j in range(2) :\n",
    "        cov[i, j] = rotate[i, j] * v_sigmas[i] * v_sigmas[j]\n",
    "eigval, eigvec = np.linalg.eig(cov)\n",
    "eigval = np.sqrt(eigval)\n",
    "\n",
    "n_data = 100000\n",
    "\n",
    "rnd_variations = np.array([np.random.normal(0, 1, n_data), np.random.normal(0, 1, n_data)]).transpose()\n",
    "fake_dataset = []\n",
    "for idx, row in enumerate(rnd_variations) :\n",
    "    dp = v_means + np.matmul(rotate, v_sigmas*row)\n",
    "    if (dp[0] > Amax) or (dp[0] < Amin) : continue\n",
    "    if (dp[1] > Bmax) or (dp[1] < Bmin) : continue\n",
    "    fake_dataset.append(dp)\n",
    "fake_dataset = np.array(fake_dataset)\n",
    "print(len(fake_dataset))\n",
    "\n",
    "plt.hist2d(fake_dataset[:,0], fake_dataset[:,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_special_encoding_constants_for_axis (dataset, axis, axmin, axmax, ax_npoints, frac_constant) :\n",
    "    tmp_dataset = dataset[:,axis]\n",
    "    ax_scan_points = np.linspace(axmin, axmax, 1+ax_npoints)\n",
    "    tmp_dataset = np.array([x for x in tmp_dataset if (x>axmin and x<axmax)])\n",
    "    \n",
    "    data_cdf = []\n",
    "    for A in ax_scan_points :\n",
    "        data_cdf.append(len([x for x in tmp_dataset if x < A]) / len(tmp_dataset))\n",
    "    data_cdf     = np.array(data_cdf)\n",
    "    constant_cdf = (ax_scan_points - axmin) / (axmax - axmin)\n",
    "    combined_cdf = frac_constant*constant_cdf + (1-frac_constant)*data_cdf\n",
    "    \n",
    "    Gauss_x   = np.linspace(-5, 5, 201)\n",
    "    Gauss_cdf = stats.norm.cdf(Gauss_x)\n",
    "    Gauss_cdf[0], Gauss_cdf[-1] = 0., 1.\n",
    "    \n",
    "    A_to_z = lambda A : np.interp(A, ax_scan_points, combined_cdf  )\n",
    "    z_to_A = lambda z : np.interp(z, combined_cdf  , ax_scan_points)\n",
    "\n",
    "    z_to_g = lambda z : np.interp(z, Gauss_cdf, Gauss_x  )\n",
    "    g_to_z = lambda g : np.interp(g, Gauss_x  , Gauss_cdf)\n",
    "\n",
    "    A_to_g = lambda A : z_to_g(A_to_z(A))\n",
    "    g_to_A = lambda g : z_to_A(g_to_z(g))\n",
    "    \n",
    "    return A_to_g, g_to_A\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def special_whiten_dataset (dataset, *axis_configs) :\n",
    "    num_axes = dataset.shape[1]\n",
    "    if num_axes != len(axis_configs) : \n",
    "        raise ValueError(f\"Dataset with shape {dataset.shape} requires {num_axes} axis configs but {len(axis_configs)} provided\")\n",
    "    whitening_funcs = []\n",
    "    for axis_idx in range(num_axes) :\n",
    "        axis_config = axis_configs[axis_idx]\n",
    "        whitening_funcs.append(get_encoding_constants_for_axis (dataset, axis_idx, axis_config[0], axis_config[1], axis_config[2], axis_config[3]))\n",
    "    white_dataset = np.array([[whitening_funcs[idx][0](x[idx]) for idx in range(num_axes)] for x in fake_dataset])\n",
    "    white_dataset, whitening_params = whiten_data(white_dataset)\n",
    "    \n",
    "    return white_dataset, whitening_funcs, whitening_params\n",
    "\n",
    "def special_unwhiten_dataset (white_dataset, whitening_funcs, whitening_params) :\n",
    "    num_axes = white_dataset.shape[1]\n",
    "    unwhite_dataset = unwhiten_data(white_dataset, whitening_params)\n",
    "    unwhite_dataset = np.array([[whitening_funcs[idx][1](x[idx]) for idx in range(num_axes)] for x in unwhite_dataset])\n",
    "    return unwhite_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_dataset, whitening_funcs, whitening_params = special_whiten_dataset(fake_dataset, \n",
    "                                                                          [Amin, Amax, A_npoints, 0.2], \n",
    "                                                                          [Bmin, Bmax, B_npoints, 0.2])\n",
    "\n",
    "unwhite_dataset = special_unwhiten_dataset(white_dataset, whitening_funcs, whitening_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 2))\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "ax.set_title(\"Original A\")\n",
    "ax.hist(fake_dataset[:,0])\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "ax.set_title(\"Original B\")\n",
    "ax.hist(fake_dataset[:,1])\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "ax.set_title(\"Original A vs B\")\n",
    "ax.hist2d(fake_dataset[:,0], fake_dataset[:,1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "ax.set_title(\"Whitened A'\")\n",
    "ax.hist(white_dataset[:,0])\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "ax.set_title(\"Whitened B'\")\n",
    "ax.hist(white_dataset[:,1])\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "ax.set_title(\"Whitened A' vs B'\")\n",
    "ax.hist2d(white_dataset[:,0], white_dataset[:,1])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "ax.set_title(\"Reconstructed A\")\n",
    "ax.hist(unwhite_dataset[:,0])\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "ax.set_title(\"Reconstructed B\")\n",
    "ax.hist(unwhite_dataset[:,1])\n",
    "\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "ax.set_title(\"Reconstructed A vs B\")\n",
    "ax.hist2d(unwhite_dataset[:,0], unwhite_dataset[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dataset = fake_dataset[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_scan_points = np.linspace(Amin, Amax, 1+A_npoints)\n",
    "\n",
    "tmp_dataset = np.array([x for x in tmp_dataset if (x>Amin and x<Amax)])\n",
    "\n",
    "data_cdf = []\n",
    "for A in A_scan_points :\n",
    "    data_cdf.append(len([x for x in tmp_dataset if x < A]) / len(tmp_dataset))\n",
    "data_cdf = np.array(data_cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_cdf = (A_scan_points - Amin) / (Amax - Amin)\n",
    "frac_constant = 0.2\n",
    "combined_cdf = frac_constant*constant_cdf + (1-frac_constant)*data_cdf\n",
    "\n",
    "plt.plot(A_scan_points, data_cdf)\n",
    "plt.plot(A_scan_points, constant_cdf)\n",
    "plt.plot(A_scan_points, combined_cdf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gauss_x   = np.linspace(-5, 5, 501)\n",
    "Gauss_cdf = stats.norm.cdf(Gauss_x)\n",
    "print(Gauss_cdf[0], Gauss_cdf[-1])\n",
    "Gauss_cdf[0], Gauss_cdf[-1] = 0., 1.\n",
    "print(\"-------->\")\n",
    "print(Gauss_cdf[0], Gauss_cdf[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_to_z = lambda A : np.interp(A, A_scan_points, combined_cdf )\n",
    "z_to_A = lambda z : np.interp(z, combined_cdf , A_scan_points)\n",
    "\n",
    "z_to_g = lambda z : np.interp(z, Gauss_cdf, Gauss_x  )\n",
    "g_to_z = lambda g : np.interp(g, Gauss_x  , Gauss_cdf)\n",
    "\n",
    "A_to_g = lambda A : z_to_g(A_to_z(A))\n",
    "g_to_A = lambda g : z_to_A(g_to_z(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tmp_dataset)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(A_to_g(tmp_dataset))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(g_to_A(A_to_g(tmp_dataset)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
